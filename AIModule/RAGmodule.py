import sys
from pathlib import Path
# è‡ªåŠ¨è®¡ç®—é¡¹ç›®æ ¹ç›®å½•è·¯å¾„ï¼ˆæ ¹æ®å®é™…æ–‡ä»¶å±‚çº§è°ƒæ•´ï¼‰
current_file = Path(__file__).resolve()
project_root = current_file.parent.parent
sys.path.insert(0, str(project_root))

import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import faiss
from functools import lru_cache
from llama_index.core import (
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
    Settings
)
from llama_index.core.schema import Document
from llama_index.core.node_parser import SentenceWindowNodeParser
from llama_index.vector_stores.faiss import FaissVectorStore
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.llms.ollama import Ollama
from ConfigModule.ConfigManager import config


class RAGProcessor:
    def __init__(self):
        # åˆå§‹åŒ–é…ç½®
        self.novel_data_dir = Path("ReferDatabase/Novels")
        self.index_dir = Path("ReferDatabase/RAG_index")
        self.index = None

        self._init_models()

        if self._index_exists():
            print("æ£€æµ‹åˆ°å·²æœ‰ç´¢å¼•ï¼Œå°è¯•åŠ è½½...")
            self._load_existing_index()
        else:
            print("æœªæ£€æµ‹åˆ°ç´¢å¼•ï¼Œåˆå§‹åŒ–æ–°å­˜å‚¨")
            self._build_index()
        self.index = load_index_from_storage(self.storage_context)
    
    def _init_models(self):
        """åˆå§‹åŒ–æ¨¡å‹é…ç½®"""
        self.embed_model = OllamaEmbedding(
            base_url="http://localhost:11434",
            model_name=config.get("embedding_model"),
        )

        self.llm = Ollama(
            base_url="http://localhost:11434",
            model=config.get("llm_model"),
            temperature=config.get("temperature", 0.7),
            request_timeout=300,
            gpu_layers=32  # å¯ç”¨GPUåŠ é€Ÿ
        )

        # åˆ†å—è§£æå™¨é…ç½®
        self.node_parser = SentenceWindowNodeParser(
            window_size=512,
            window_metadata_key="context_window",
            original_text_metadata_key="original_content",
        )
        
        # å…¨å±€è®¾ç½®
        Settings.embed_model = self.embed_model
        Settings.llm = self.llm
        Settings.chunk_size = config.get("chunk_size", 512)
        Settings.chunk_overlap = 128

    def _index_exists(self) -> bool:
        """éªŒè¯ç´¢å¼•æ–‡ä»¶å®Œæ•´æ€§"""
        required_files = [
            "docstore.json",    # LlamaIndexçš„æ–‡æ¡£å­˜å‚¨
            "faiss_index.bin",  # FaissäºŒè¿›åˆ¶ç´¢å¼•æ–‡ä»¶
            "version.info"      # ç‰ˆæœ¬ä¿¡æ¯æ–‡ä»¶
        ]
        
        # æ£€æŸ¥æ‰€æœ‰å¿…éœ€æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        return all(
            (self.index_dir / filename).exists() 
            for filename in required_files
        )

    def _load_existing_index(self) -> None:
        """åŠ è½½å·²æœ‰ç´¢å¼•çš„å®Œæ•´æµç¨‹"""
        try:
            # 1. åŠ è½½ç‰ˆæœ¬ä¿¡æ¯
            version_file = self.index_dir / "version.info"
            with open(version_file, "r") as f:
                version_info = dict(line.strip().split(":") for line in f)
            
            # 2. æ ¡éªŒç»´åº¦ä¸€è‡´æ€§
            current_embed_dim = len(self.embed_model.get_text_embedding("test"))
            if current_embed_dim != int(version_info["embed_dim"]):
                raise ValueError(
                    f"ç»´åº¦ä¸åŒ¹é…: å½“å‰æ¨¡å‹ç»´åº¦ {current_embed_dim} "
                    f"vs ç´¢å¼•ç»´åº¦ {version_info['embed_dim']}"
                )

            # 3. åŠ è½½Faissç´¢å¼•
            faiss_index = faiss.read_index(
                str(self.index_dir / "faiss_index.bin"),
                faiss.IO_FLAG_MMAP  # å†…å­˜æ˜ å°„æ¨¡å¼ï¼Œé€‚åˆå¤§æ–‡ä»¶
            )
            
            # 4. é‡å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
            self.vector_store = FaissVectorStore(faiss_index=faiss_index)
            self.storage_context = StorageContext.from_defaults(
                vector_store=self.vector_store,
                persist_dir=self.index_dir
            )
        except Exception as e:
            print(f"âŒ ç´¢å¼•åŠ è½½å¤±è´¥: {str(e)}")
            print("âš ï¸ æ­£åœ¨å°è¯•é‡å»ºç´¢å¼•...")
            self._build_index(force_rebuild=True)

    def _init_faiss_store(self):
        """åˆå§‹åŒ–Faisså­˜å‚¨"""
        # å®šä¹‰å‘é‡ç»´åº¦ï¼ˆéœ€ä¸åµŒå…¥æ¨¡å‹è¾“å‡ºç»´åº¦ä¸€è‡´ï¼‰
        test_embedding = self.embed_model.get_text_embedding("test")
        self.embed_dim = len(test_embedding)
        print("embed_model å‘é‡ç»´åº¦")
        print(self.embed_dim)
        
        # åˆ›å»ºFaissç´¢å¼•ï¼ˆHNSWç®—æ³•ï¼‰
        faiss_index = faiss.IndexHNSWFlat(self.embed_dim, 32)
        faiss_index.hnsw.efConstruction = 200
        faiss_index.hnsw.efSearch = 128
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        self.vector_store = FaissVectorStore(faiss_index=faiss_index)
        
        # æ›´æ–°å­˜å‚¨ä¸Šä¸‹æ–‡
        self.storage_context = StorageContext.from_defaults(
            vector_store=self.vector_store
        )

    def _process_large_txt(self, file_path: Path) -> List[Document]:
        """æµå¼å¤„ç†å¤§å‹TXTæ–‡ä»¶"""
        documents = []
        chunk_id = 0
        buffer = []
        buffer_size = 0  # æŒ‰å­—èŠ‚è®¡ç®—
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                buffer.append(line)
                buffer_size += len(line.encode('utf-8'))
                
                # æ¯ç§¯ç´¯500KBå¤„ç†ä¸€æ¬¡
                if buffer_size >= 512 * 1024:
                    text_chunk = "".join(buffer)
                    documents.append(Document(
                        text=text_chunk,
                        metadata={
                            "source": str(file_path),
                            "chunk_id": chunk_id,
                            "total_chunks": "processing",
                            "type": "content",
                            "create_time": datetime.now().isoformat()
                        }
                    ))
                    chunk_id += 1
                    buffer = []
                    buffer_size = 0
            
            # å¤„ç†å‰©ä½™å†…å®¹
            if buffer:
                text_chunk = "".join(buffer)
                documents.append(Document(
                    text=text_chunk,
                    metadata={
                        "source": str(file_path),
                        "chunk_id": chunk_id,
                        "total_chunks": chunk_id + 1,
                        "type": "content",
                        "create_time": datetime.now().isoformat()
                    }
                ))
        return documents

    def _load_documents(self) -> List[Document]:
        """åŠ è½½æ‰€æœ‰æ–‡æ¡£æ•°æ®"""
        documents = []
        
        # åŠ è½½TXTæ–‡ä»¶
        for txt_file in self.novel_data_dir.glob("*.txt"):
            if txt_file.stat().st_size > 50 * 1024 * 1024:  # 50MBä»¥ä¸Šå¤§æ–‡ä»¶
                documents.extend(self._process_large_txt(txt_file))
            else:
                with open(txt_file, 'r', encoding='utf-8') as f:
                    documents.append(Document(
                        text=f.read(),
                        metadata={
                            "source": str(txt_file),
                            "type": "content",
                            "create_time": datetime.now().isoformat()
                        }
                    ))
        
        return documents

    def _build_index(self, force_rebuild: bool = False) -> None:
        self._init_faiss_store()

        """æ„å»º/æ›´æ–°ç´¢å¼•"""
        if not force_rebuild and (self.index_dir / "docstore.json").exists():
            print("ğŸ”„ æ£€æµ‹åˆ°å·²æœ‰ç´¢å¼•ï¼Œä½¿ç”¨ç¼“å­˜")
            self._load_existing_index()
            return

        print("â³ å¼€å§‹æ„å»ºç´¢å¼•...")
        documents = self._load_documents()
        if not documents:
            raise ValueError("æœªæ‰¾åˆ°å¯ç´¢å¼•çš„æ•°æ®æ–‡ä»¶")
        
        # åˆ›å»ºç´¢å¼•
        VectorStoreIndex.from_documents(
            documents,
            storage_context=self.storage_context,
            show_progress=True
        )
        
        # æŒä¹…åŒ–ç´¢å¼•
        self._persist_faiss_index()
    
    def _persist_faiss_index(self):
        """æŒä¹…åŒ–Faissç´¢å¼•"""
        try:
            self.index_dir.mkdir(parents=True, exist_ok=True)
            
            if not (self.index_dir / "docstore.json").exists():
                # ä¿å­˜LlamaIndexå…ƒæ•°æ®
                self.storage_context.persist(persist_dir=self.index_dir)
            
            # ä¿å­˜Faissç´¢å¼•
            faiss.write_index(
                self.vector_store.client,
                str(self.index_dir / "faiss_index.bin")
            )
            
            # è®°å½•ç‰ˆæœ¬ä¿¡æ¯
            with open(self.index_dir / "version.info", "w") as f:
                f.write(f"faiss_version:{faiss.__version__}\n")
                f.write(f"embed_dim:{self.embed_dim}\n")
                
            print(f"ğŸ’¾ ç´¢å¼•æŒä¹…åŒ–å®Œæˆï¼ˆç»´åº¦ï¼š{self.embed_dim}ï¼‰")
            
        except Exception as e:
            print(f"âŒ æŒä¹…åŒ–å¤±è´¥: {str(e)}")
            raise

    def get_query_engine(self, similarity_top_k: int = 5):
        """è·å–æŸ¥è¯¢å¼•æ“"""
        if not self.index:
            raise RuntimeError("ç´¢å¼•æœªæ­£ç¡®åˆå§‹åŒ–")
        
        return self.index.as_query_engine(
            similarity_top_k=similarity_top_k,
            response_mode="tree_summarize",
            verbose=True
        )

    @lru_cache(maxsize=1000)  # ç¼“å­˜1000æ¡æŸ¥è¯¢
    def GenerateWithOllama(self, prompt: str, top_k: int = 5) -> str:
        print("RAG GenerateWithOllama")
        """æ‰§è¡ŒRAGæŸ¥è¯¢"""
        try:
            query_engine = self.get_query_engine(top_k)
            response = query_engine.query(prompt)
            return str(response).strip()
        except Exception as e:
            return f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}"
        

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = RAGProcessor()
    
    # ç¤ºä¾‹æŸ¥è¯¢
    test_queries = [
        "å°è¯´çš„ä¸»è§’æœ‰å“ªäº›ç‰¹æ®Šèƒ½åŠ›ï¼Ÿ",
        "æ•…äº‹çš„ä¸»è¦å†²çªæ˜¯ä»€ä¹ˆï¼Ÿ",
        "åˆ—å‡ºæ‰€æœ‰ä¿®ä»™ç›¸å…³çš„è®¾å®š"
    ]
    
    common_prefix = "è¯·ä»¥ä¸­æ–‡å›ç­”ï¼Œå¹¶å°½é‡è¯¦ç»†ã€‚"

    for query in test_queries:
        print(f"\nğŸ” æŸ¥è¯¢: {query}")
        print("ğŸ“– å“åº”:", processor.GenerateWithOllama(common_prefix + query))
        print("â”" * 50)

