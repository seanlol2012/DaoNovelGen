import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from llama_index.core import (
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
    Settings
)
from llama_index.core.schema import Document
from llama_index.core.node_parser import SentenceWindowNodeParser
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.llms.ollama import Ollama
from ConfigModule.ConfigManager import config


class RAGProcessor:
    def __init__(self):
        # åˆå§‹åŒ–é…ç½®
        self.novel_data_dir = Path("ReferDatabase/Novels")
        self.index_dir = Path("ReferDatabase/RAG_index")
        
        self.embed_model = OllamaEmbedding(
            model_name=config.get("embedding_model", "bge-m3:latest"),
            base_url="http://localhost:11434",
        )

        self.llm = Ollama(
            base_url="http://localhost:11434",
            model=config.get("llm_model"),
            temperature=config.get("temperature", 0.7),
            request_timeout=300
        )
        
        # åˆ†å—è§£æå™¨é…ç½®
        self.node_parser = SentenceWindowNodeParser(
            window_size=512,
            window_metadata_key="context_window",
            original_text_metadata_key="original_content",
        )
        
        Settings.embed_model = self.embed_model
        Settings.llm = self.llm
        Settings.chunk_size = config.get("chunk_size", 512)
        Settings.chunk_overlap = 128
    
    def _process_large_txt(self, file_path: Path) -> List[Document]:
        """æµå¼å¤„ç†å¤§å‹TXTæ–‡ä»¶"""
        documents = []
        chunk_id = 0
        buffer = []
        buffer_size = 0  # æŒ‰å­—èŠ‚è®¡ç®—
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                buffer.append(line)
                buffer_size += len(line.encode('utf-8'))
                
                # æ¯ç§¯ç´¯500KBå¤„ç†ä¸€æ¬¡
                if buffer_size >= 512 * 1024:
                    text_chunk = "".join(buffer)
                    documents.append(Document(
                        text=text_chunk,
                        metadata={
                            "source": str(file_path),
                            "chunk_id": chunk_id,
                            "total_chunks": "processing",
                            "type": "content",
                            "create_time": datetime.now().isoformat()
                        }
                    ))
                    chunk_id += 1
                    buffer = []
                    buffer_size = 0
            
            # å¤„ç†å‰©ä½™å†…å®¹
            if buffer:
                text_chunk = "".join(buffer)
                documents.append(Document(
                    text=text_chunk,
                    metadata={
                        "source": str(file_path),
                        "chunk_id": chunk_id,
                        "total_chunks": chunk_id + 1,
                        "type": "content",
                        "create_time": datetime.now().isoformat()
                    }
                ))
        return documents

    def _load_documents(self) -> List[Document]:
        """åŠ è½½æ‰€æœ‰æ–‡æ¡£æ•°æ®"""
        documents = []
        
        # åŠ è½½TXTæ–‡ä»¶
        for txt_file in self.novel_data_dir.glob("*.txt"):
            if txt_file.stat().st_size > 50 * 1024 * 1024:  # 50MBä»¥ä¸Šå¤§æ–‡ä»¶
                documents.extend(self._process_large_txt(txt_file))
            else:
                with open(txt_file, 'r', encoding='utf-8') as f:
                    documents.append(Document(
                        text=f.read(),
                        metadata={
                            "source": str(txt_file),
                            "type": "content",
                            "create_time": datetime.now().isoformat()
                        }
                    ))
        
        return documents

    def build_index(self, force_rebuild: bool = False) -> None:
        """æ„å»º/æ›´æ–°ç´¢å¼•"""
        if not force_rebuild and (self.index_dir / "docstore.json").exists():
            print("ğŸ”„ æ£€æµ‹åˆ°å·²æœ‰ç´¢å¼•ï¼Œä½¿ç”¨ç¼“å­˜")
            return
            
        print("â³ å¼€å§‹æ„å»ºç´¢å¼•...")
        documents = self._load_documents()
        if not documents:
            raise ValueError("æœªæ‰¾åˆ°å¯ç´¢å¼•çš„æ•°æ®æ–‡ä»¶")
        
        # åˆ›å»ºç´¢å¼•
        index = VectorStoreIndex.from_documents(
            documents,
            storage_context=StorageContext.from_defaults(),
            show_progress=True
        )
        
        # æŒä¹…åŒ–å­˜å‚¨
        self.index_dir.mkdir(parents=True, exist_ok=True)
        index.storage_context.persist(persist_dir=self.index_dir)
        print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼Œä¿å­˜è‡³ {self.index_dir}")

    def get_query_engine(self, similarity_top_k: int = 5):
        """è·å–æŸ¥è¯¢å¼•æ“"""
        if not (self.index_dir / "docstore.json").exists():
            self.build_index()
            
        storage_context = StorageContext.from_defaults(
            persist_dir=self.index_dir
        )
        
        index = load_index_from_storage(
            storage_context=storage_context,
        )
        
        return index.as_query_engine(
            similarity_top_k=similarity_top_k,
            response_mode="tree_summarize"
        )

    def GenerateWithOllama(self, prompt: str, top_k: int = 5) -> str:
        print("RAG GenerateWithOllama")
        """æ‰§è¡ŒRAGæŸ¥è¯¢"""
        try:
            query_engine = self.get_query_engine(top_k)
            response = query_engine.query(prompt)
            return str(response).strip()
        except Exception as e:
            return f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}"
        

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = RAGProcessor()
    
    # æ„å»ºç´¢å¼•ï¼ˆé¦–æ¬¡è¿è¡Œä¼šè¾ƒæ…¢ï¼‰
    processor.build_index()
    
    # ç¤ºä¾‹æŸ¥è¯¢
    test_queries = [
        "å°è¯´çš„ä¸»è§’æœ‰å“ªäº›ç‰¹æ®Šèƒ½åŠ›ï¼Ÿ",
        "æ•…äº‹çš„ä¸»è¦å†²çªæ˜¯ä»€ä¹ˆï¼Ÿ",
        "åˆ—å‡ºæ‰€æœ‰ä¿®ä»™ç›¸å…³çš„è®¾å®š"
    ]
    
    common_prefix = "è¯·ä»¥ä¸­æ–‡å›ç­”ï¼Œå¹¶å°½é‡è¯¦ç»†ã€‚"

    for query in test_queries:
        print(f"\nğŸ” æŸ¥è¯¢: {query}")
        print("ğŸ“– å“åº”:", processor.GenerateWithOllama(common_prefix + query))
        print("â”" * 50)

